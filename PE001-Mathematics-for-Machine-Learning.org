* PE001 MATHEMATICS FOR MACHINE LEARNING
:properties:
:author: R S Milton, T T Mirnalinee
:date:
:end:

#+startup: showall

{{{credits}}}
| L | T | P | C |
| 3 | 0 | 0 | 3 |

** COURSE OBJECTIVES
   - To learn the use of linear algebra for framing learning problems
   - To understand matrix decompositions
   - To apply analytic geometry and vector calculus
   - To use probability distributions
   - To apply continuous optimization

{{{unit}}}
|UNIT I |  LINEAR ALGEBRA | 9  |
Linear Algebra: Systems of Linear Equations Matrices --
Solving Systems of Linear Equations -- Vector Spaces --
Linear Independence -- Basis and Rank -- Linear Mappings --
Affine Spaces.

{{{unit}}}
| UNIT II | ANALYTIC GEOMETRY | 9  |
Analytic Geometry: Norms -- Inner Products -- Lengths and
Distances -- Angles and Orthogonality -- Orthonormal Basis --
Orthogonal Complement -- Inner Product of Functions --
Orthogonal Projections -- Rotations

{{{unit}}}
| UNIT III | MATRIX DECOMPOSITIONS | 9  |
Matrix Decompositions: Determinant and Trace -- Eigenvalues
and Eigenvectors -- Cholesky Decomposition --
Eigendecomposition and Diagonalization -- Singular Value
Decomposition -- Matrix Approximation -- Matrix Phylogeny

{{{unit}}}
| UNIT IV | VECTOR CALCULUS | 9  |
Vector Calculus: Differentiation of Univariate Functions --
Partial Differentiation and Gradients -- Gradients of
Vector-Valued Functions -- Gradients of Matrices -- Useful
Identities for Computing Gradients -- Backpropagation and
Automatic Differentiation -- Higher-Order Derivatives --
Linearization and Multivariate Taylor Series

{{{unit}}}
| UNIT V | PROBABILITY AND DISTRIBUTIONS, CONTINUOUS OPTIMIZATION | 9 |
Probability and Distributions: Construction of a Probability
Space -- Discrete and Continuous Probabilities -- Sum Rule,
Product Rule, and Bayes' Theorem -- Summary Statistics and
Independence -- Gaussian Distribution -- Conjugacy and the
Exponential Family -- Change of Variables/Inverse Transform;
Continuous Optimization: Optimization Using Gradient Descent
-- Constrained Optimization and Lagrange Multipliers --
Convex Optimization

\hfill *Total Periods: 45*

** COURSE OUTCOMES
After the completion of this course, students will be able to
1. Solve learning problems using linear algebra  (K3)
2. Apply analytic geometry for handling multi-dimensional data (K3)
3. Construct matrix decompositions (K3)
4. Apply vector calculus for optimization problems (K3)  
5. Choose appropriate probability distributions and apply continuous
   optimization (K3).


** TEXT BOOKS
1. Marc Peter Deisenroth, A Aldo Faisal, Cheng Soon Ong,
   ``Mathematics for Machine Learning'', Cambridge University
   Press, 2020

** REFERENCES
1. David J C MacKay, ``InformationTheory, Inference,and
   Learning Algorithms'', Cambridge University Press, 2003.
2. Christopher Bishop, ``Pattern Recognition and Machine
   Learning'', Springer,2006

** CO TO PO/PSO MAPPING
|        | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 1 | 2 | 3 |
|--------+---+---+---+---+---+---+---+---+---+----+----+----+---+---+---|
| CO1    | 3 | 2 | 1 | 1 | 2 | 0 | 0 | 0 | 1 |  1 |  0 |  1 | 3 | 0 | 0 |
| CO2    | 3 | 2 | 1 | 1 | 2 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 3 | 0 | 0 |
| CO3    | 3 | 2 | 1 | 1 | 2 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 2 | 0 | 0 |
| CO4    | 3 | 2 | 1 | 1 | 2 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 3 | 0 | 0 |
| CO5    | 3 | 2 | 2 | 1 | 2 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 3 | 0 | 0 |
|--------+---+---+---+---+---+---+---+---+---+----+----+----+---+---+---|
| Course | 3 | 2 | 2 | 1 | 2 | 0 | 0 | 0 | 1 |  1 |  0 |  1 | 3 | 0 | 0 |

#+tblfm: @>>$2..@>>$>='(apply '+ '(@<<..@>>>));N      
#+tblfm: @>$2..@>$>='(ceiling (/ (* 1.0 (apply '+ '(@<<..@>>>)))(length '(@<<..@>>>))));N      
# | Score | 15 | 10 | 6 | 5 | 10 | 0 | 0 | 0 | 1 | 5 | 0 | 5 | 14 | 0 | 0 |
