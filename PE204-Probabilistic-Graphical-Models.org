* <<<PE204>>> PROBABILISTIC GRAPHICAL MODELS
:properties:
:author: Dr.R.S.Milton, Ms.S.Rajalakshmi
:date: 9.3.21
:end:

#+begin_comment
Included project model in Unit I instead of having it in AU R2017 -Unit II
Included scheduling in Unit II instead of having it in AU R2017-Unit III
Added security topic in syllabus, which is not provided in AU R2017
Added risk in Unit IV instead of AU R2017-Unit III
New process model is added in syllabus which is not provided in AU R2017
Included monitoring topics in Unit V instead of AU-Unit IV
#+end_comment

#+startup: showall
** CO PO MAPPING :noexport:
#+NAME: co-po-mapping
|                |    | PO1 | PO2 | PO3 | PO4 | PO5 | PO6 | PO7 | PO8 | PO9 | PO10 | PO11 | PO12 | PSO1 | PSO2 | PSO3 |
|                |    |  K3 |  K4 |  K5 |  K5 |  K6 |   - |   - |   - |   - |    - |    - |    - |   K5 |   K3 |   K6 |
| CO1            | K2 |   3 |   3 |   1 |   1 |   1 |   0 |   0 |   0 |   1 |    0 |    0 |    1 |    1 |    1 |    1 |
| CO2            | K3 |   3 |   3 |   3 |   3 |   1 |   0 |   0 |   0 |   1 |    0 |    0 |    1 |    3 |    2 |    3 |
| CO3            | K4 |   3 |   3 |   3 |   3 |   1 |   0 |   0 |   0 |   1 |    0 |    0 |    1 |    3 |    2 |    3 |
| CO4            | K4 |   3 |   3 |   3 |   3 |   1 |   0 |   0 |   0 |   1 |    0 |    0 |    1 |    3 |    2 |    3 |
| CO5            | K3 |   3 |   3 |   3 |   3 |   1 |   0 |   0 |   0 |   1 |    0 |    0 |    1 |    3 |    2 |    3 |
| CO6            | K3 |   3 |   3 |   3 |   3 |   3 |   0 |   0 |   1 |   3 |    3 |    0 |    1 |    3 |    3 |    3 |
| Score          |    |  18 |  18 |  16 |  16 |   8 |   0 |   0 |   1 |   8 |    3 |    0 |    5 |   16 |   12 |   16 |
| Course Mapping |    |   3 |   3 |   3 |   3 |   2 |   0 |   0 |   1 |   2 |    1 |    0 |    1 |    3 |    3 |    3 |

{{{credits}}}
| L | T | P | C |
| 3 | 0 | 0 | 3 |

** COURSE OBJECTIVES
- To learn the key aspects of directed and undirected models
- To apply the techniques to represent the model, do inference and learning
- To understand the methods for learning in hidden data.
#+begin_comment
...Included project model in Unit I instead of having it in AU-Unit II...
#+end_comment

{{{unit}}}
|UNIT I | INTRODUCTION| 9 |
Probabilistic Reasoning: Conditional Probability -- Probability Tables
-- Prior, Likelihood and Posterior; Graph Concept; Belief Networks:
Benefits of structure -- Uncertain and Unreliable Evidence -- Belief
concepts -- Causality.

{{{unit}}}
|UNIT II | REPRESENTATION IN GRAPHICAL MODELS | 9 |
Bayesian Network Representation: Independence properties --
Independence in graphs -- From distributions to graphs; Undirected
Graphical Models: Parameterization -- Markov Network Independencies --
Bayesian networks and Markov networks -- Partially directed models.

{{{unit}}}
|UNIT III | INFERENCE IN GRAPHICAL MODELS | 9 |
Efficient Inference in Trees: Marginal Inference -- Forms of Inference
-- Inference in Multiply Connected Graphs; Junction Tree Algorithm:
Clustering variables -- Clique graphs -- Junction trees --
Constructing junction trees for singly-Connected Distributions --
Junction Trees for Multiply-Connected Distributions -- Junction Tree
Algorithm; Making Decisions: Expected Utility -- Extending Bayesian
Networks for Decisions -- Solving Influence Diagrams -- Markov
Decision Processes -- Variational Inference and Planning.

{{{unit}}}
|UNIT IV | LEARNING IN PROBABILISTIC MODELS | 9 |
Statistics for Machine Learning: Representing Data -- Distributions --
Multivariate Gaussian -- Conjugate priors -- Properties of Maximum
Likelihood -- Learning a Gaussian; Learning as Inference: Bayesian
methods -- Maximum Likelihood Training of Belief Networks -- Bayesian
Belief Network Training -- Structure learning -- Maximum Likelihood
for Undirected models; Naive Bayes: Conditional Independence --
Estimation using Maximum Likelihood -- Bayesian Naive Bayes -- Tree
Augmented Naive Bayes.

{{{unit}}}
|UNIT V | LEARNING IN HIDDEN ENVIRONMENT | 9 |
Learning with Hidden Variables: Hidden Variables and Missing Data --
Expectation Maximisation -- Extensions of EM -- Variational Bayes;
Bayesian Model Selection: Comparing Models the Bayesian Way -- Occamâ€™s
Razor and Bayesian Complexity Penalisation -- Approximating the Model
Likelihood -- Bayesian Hypothesis Testing for Outcome Analysis.


** COURSE OUTCOMES
Upon completion of the course, the student should be able to:
- Explain the need for probabilistic graphical models (K2)
- Apply and analyze the various representations like directed and undirected models (K3)
- Select the inference algorithms to analyze the models  (K3)
- Apply the learning techniques to learn the structure and parameter in models (K3)
- Identify the learning techniques for hidden data (K2).
      
** TEXT BOOKS
1. David Barber, ``Bayesian Reasoning and Machine Learning'',
   Cambridge University Press, 2020. (Unit I,III,IV,V)
2. Daphne Koller, Nir Friedman, ``Probabilistic Graphical Models -
   Principles and Technique'', MIT Press, 2007. (Unit II)

** REFERENCES
1. Luis Enrique Sucar, ``Probabilistic Graphical Models - Principles
   and Applications'', Advances in Computer Vision and Pattern
   Recognition, Springer, 2015.
2. Kiran R Karkera, ``Building Probabilistic Graphical Models with
   Python'', Packt Publishing, 2014.

