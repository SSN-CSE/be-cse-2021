* <<<PE205>>> NATURAL LANGUAGE PROCESSING
:properties:
:author: Dr. D. THenmozhi and Mr. B. Senthil Kumar
:date: 09-03-2021
:end:

#+begin_comment
- 1. Combined Unit 2 and 3 of AU into Unit 2, Unit 4 and 5 of AU into Unit 3 to give emphasis on
   NLP applications
- 2. For changes, see the indidual units
- 3. The unit headings are similar to M.E syllabus with addition and deletion of topics except Unit 4. 
     Unit 4 and 5 are focussing on NLP applications. Removed NLP using Python
- 4. Five Course outcomes specified and aligned with units
- 5. Not Applicable
#+end_comment

#+startup: showall
{{{credits}}}
|L|T|P|C|
|3|0|0|3|

** CO PO MAPPING :noexport:
#+NAME: co-po-mapping
|                |    | PO1 | PO2 | PO3 | PO4 | PO5 | PO6 | PO7 | PO8 | PO9 | PO10 | PO11 | PO12 | PSO1 | PSO2 | PSO3 |
| CO1            | K3 |   3 |   3 |  0 |   0 |  0 |   0 |   0 |  0 |   0 |   1 |    0 |    3 |    3 |    0 |   0 |
| CO2            | K2 |   2 |  3  |  0 |   0 |  0 |   0 |   0 |  0 |   0 |   1 |    0 |    3 |    3 |    0 |    1 |
| CO3            | K3 |   1 |  1  |  0 |   0 |  0 |   0 |   0 |  0 |   0 |   1 |    0 |    3 |    3 |    0 |    1 |
| CO4            | K3 |   3 |  3  |  0 |   0 |  0 |   0 |   0 |  0 |   0 |   1 |    0 |    3 |    3 |    0 |    1 |
| CO5            | K3 |   1 |  2  |  0 |   0 |  0 |   0 |   0 |  0 |   0 |   1 |    0 |    3 |    3 |    0 |    1 |
| CO6            | K4 |   3 |   3 |  3 |   3 |  2 |   0 |   0 |  1 |   3 |   2 |    0 |    3 |    3 |    2 |    3 |
| Score          |    |  13 |  15 |  3 |   3 |   2 |   0 |   0 |  1 |  3 |   7 |    0 |   18 |   18 |   2 |  7 |
| Course Mapping |    |   3 |   3 |   1 |   1 |   1 |   0 |   0 |   1 |   1 |    1 |   0 |   3 |   3 |    1 |  1 |

** COURSE OBJECTIVES
- To learn language models
- To learn text pre processing techniques
- To understand the levels of knowledge in language processing
- To develop NLP applications.
- To apply traditional learning and deep learning for NLP applications.

{{{unit}}}
| UNIT I | TEXT PRE-PROCESSING AND LANGUAGE MODELLING | 9 |
Knowledge in language processing -- NLP applications; -- Regular Expressions -- Words -- 
Corpora -- Text Normalization -- Minimum Edit distance -- N-gram language models -- 
Neural language models - RNNs as language models


#+begin_comment

- 1. Removed grammar based language models
- 2. Added Neural language models
- 3. Moved text pre processing from Unit II to Unit 1

#+end_comment

{{{unit}}}
| UNIT II | WORD LEVEL AND SYNTACTIC ANALYSIS | 9 |
Word Level Analysis: Word classes -- Part-of-Speech Tagging: HMM POS tagging; Named Entities (NE): NE Tagging -- 
Conditional Random Field NE recognizer; Syntactic Analysis: Constituency -- Context-free grammar 
-- Grammar rules -- Treebanks; Parsing: Top-down -- Bottom-up -- Ambiguity -- CKY Parsing -- 
Shallow parsing -- Dependency parsing 


#+begin_comment

- 1. Removed Early algorithm
- 2. Added Shallow parsing
- 3. Moved pre processing to Unit I from Unit II
- 4. Added NE tagging in word level analysis
#+end_comment


{{{unit}}}
| UNIT III | SEMANTIC ANALYSIS | 9 |
Vector Semantics -- Words and Vectors -- Cosine similarity -- Tf-idf -- Positive PMI -- Word2vec-- 
Semantic properties of embeddings; Lexical Semantics: Word Senses -- Relations between senses -- 
WordNet -- Word Sense Disambiguation


#+begin_comment

- 1. Removed basic representations of semantics
- 2. Added Vector semantics
- 3. Removed thematic roles from lexical semantics
- 4. Added Word embeddings

#+end_comment

{{{unit}}}

| UNIT IV | COREFERENCE RESOLUTION AND MACHINE TRANSLATION  | 9 |
Coreference Resolution: Coreference phenomena -- Mention detection -- Mention-pair architecture;
RNNs for sequence labeling and classification --  Stacked and Bi-directional RNN -- Machine Translation(MT): 
Lexical divergence and typology -- Encoder-Decoder with RNNs --  MT Evaluation; 



#+begin_comment

- 1. Added Mention detection
- 2. Removed Centering and other basic algorithms for reference resolution
- 3. Added deep learning for sequence labeling and classification
- 4. Moved machine translation from Unit V to Unit IV
#+end_comment

{{{unit}}}
| UNIT V | NLP Applications | 9 |
Sentiment Classification: Naive Bayes classifier -- Optimizing for Sentiment Analysis -- Evaluation; 
Information Extraction: Relation extraction; Information Retrieval ; IR-based Factoid Question Answering: 
IR-based QA Datasets -- Answer span extraction; 



#+begin_comment

- 1. Moved IR and IE from Unit IV to Unit V
- 2. Added Sentiment analysis

#+end_comment


NLP Tasks / Applications:
1) CRF POS/NER Tagging
2) Word Generation using N-grams
3) CFG / Dependency parsing
4) Semantics of Word2vec embeddings
5) Neural machine translation - NMT
6) Sequence classification / Naive Bayes classifier for Sentiment analysis
7) Relation Extraction



\hfill *Total Periods: 45*

** COURSE OUTCOMES
Upon the completion of the course the students should be able to: 
- Apply text pre processing techniques and build the language models (K3)
- Explain the basic levels of knowledge namely word level and syntax level in language processing (K2)
- Apply computational methods in lexical and vector semantics (K3)
- Apply NLP techniques for discourse processing and build machine translation systems using deep learning (K3)
- Apply learning methodologies for different NLP applications (K3).
- Develop any NLP application by using NLP techniques and learning methodologies (K4)

** TEXT BOOKS
1. Daniel Jurafsky and James H Martin, ``Speech and Language
   Processing: An introduction to Natural Language Processing,
   Computational Linguistics and Speech Recognition'', 2nd Edition,
   Prentice Hall, 2008.
2. https://web.stanford.edu/~jurafsky/slp3/


** REFERENCES
1. Christopher D Manning, Hinrich Schutze, ``Foundations of
   Statistical Natural Language Processing'', MIT Press, 1999.
2. Steven Bird, Ewan Klien and Edward Loper, Natural Language Processing with Python,
   O'Reilly, 2009.
3. Nitin Indurkhya, Fred J Damerau, "Handbook of Natural Language
   Processing", 2nd Edition, CRC Press, 2010.
4. Yoav Goldberg, "Neural Network Methods for Natural Language
   Processing", Synthesis Lectures on Human Language Technologies,
   Morgan & Claypool publishers, 2017.
5. Li Deng, Yang Liu, "Deep Learning in Natural Language Processing", 
   Springer, 2018
6. Taweh Beysolow II, "Applied Natural Language Processing - Implementing 
   Machine Learning and Deep Learning Algorithms for Natural Language Processing", 
   Apress, 2018
7. NLTK -- Natural Language Tool Kit - http://www.nltk.org/
8. http://nlp-iiith.vlabs.ac.in/
9. https://www.tensorflow.org/tutorials/text/nmt_with_attention
