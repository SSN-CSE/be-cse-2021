* 
:properties:
:author: Dr. D. Thenmozhi and Mr. B. Senthil Kumar
:date: 09-03-2021
:end:

#+startup: showall
{{{title-tab}}}
| CODE    | COURSE TITLE                                 | L | T | P | E | C |
| UCS2627 | NATURAL LANGUAGE PROCESSING AND APPLICATIONS | 3 | 0 | 0 | 0 | 3 |

** R2021 CHANGES :noexport:
1. Combined Unit 2 and 3 of AU into Unit 2, Unit 4 and 5 of AU into
   Unit 3 to give emphasis on NLP applications
2. For changes, see the indidual units
3. The unit headings are similar to M.E syllabus with addition and
   deletion of topics except Unit 4.  Unit 4 and 5 are focussing on
   NLP applications. Removed NLP using Python
4. Five Course outcomes specified and aligned with units
5. Not Applicable


** COURSE OBJECTIVES
- To learn language models
- To learn text pre processing techniques
- To understand the levels of knowledge in language processing
- To develop NLP applications.
- To apply traditional learning and deep learning for NLP applications.

{{{unit}}}
| UNIT I | TEXT PRE-PROCESSING AND LANGUAGE MODELLING | 9 |
Knowledge in language processing -- NLP applications; -- Regular Expressions -- Words -- 
Corpora -- Text Normalization -- Minimum Edit distance -- N-gram language models -- 
Neural language models - RNNs as language models


#+begin_comment

- 1. Removed grammar based language models
- 2. Added Neural language models
- 3. Moved text pre processing from Unit II to Unit 1

#+end_comment

{{{unit}}}
| UNIT II | WORD LEVEL AND SYNTACTIC ANALYSIS | 9 |
Word Level Analysis: Word classes -- Part-of-Speech Tagging: HMM POS tagging; Named Entities (NE): NE Tagging -- 
Conditional Random Field NE recognizer; Syntactic Analysis: Constituency -- Context-free grammar 
-- Grammar rules -- Treebanks; Parsing: Top-down -- Bottom-up -- Ambiguity -- CKY Parsing -- 
Shallow parsing -- Dependency parsing 


#+begin_comment

- 1. Removed Early algorithm
- 2. Added Shallow parsing
- 3. Moved pre processing to Unit I from Unit II
- 4. Added NE tagging in word level analysis
#+end_comment


{{{unit}}}
| UNIT III | SEMANTIC ANALYSIS | 9 |
Vector Semantics -- Words and Vectors -- Cosine similarity -- Tf-idf -- Positive PMI -- Word2vec-- 
Semantic properties of embeddings; Lexical Semantics: Word Senses -- Relations between senses -- 
WordNet -- Word Sense Disambiguation


#+begin_comment
- 1. Removed basic representations of semantics
- 2. Added Vector semantics
- 3. Removed thematic roles from lexical semantics
- 4. Added Word embeddings
#+end_comment

{{{unit}}}
| UNIT IV | COREFERENCE RESOLUTION AND MACHINE TRANSLATION  | 9 |
Coreference Resolution: Coreference phenomena -- Mention detection -- Mention-pair architecture;
RNNs for sequence labeling and classification --  Stacked and Bi-directional RNN -- Machine Translation(MT): 
Lexical divergence and typology -- Encoder-Decoder with RNNs --  MT Evaluation; 

#+begin_comment
- 1. Added Mention detection
- 2. Removed Centering and other basic algorithms for reference resolution
- 3. Added deep learning for sequence labeling and classification
- 4. Moved machine translation from Unit V to Unit IV
#+end_comment

{{{unit}}}
| UNIT V | NLP APPLICATIONS | 9 |
Sentiment Classification: Naive Bayes classifier -- Optimizing for Sentiment Analysis -- Evaluation; 
Information Extraction: Relation extraction; Information Retrieval ; IR-based Factoid Question Answering: 
IR-based QA Datasets -- Answer span extraction; 

#+begin_comment
- 1. Moved IR and IE from Unit IV to Unit V
- 2. Added Sentiment analysis
#+end_comment

NLP Tasks / Applications:
1) CRF POS/NER Tagging
2) Word Generation using N-grams
3) CFG / Dependency parsing
4) Semantics of Word2vec embeddings
5) Neural machine translation - NMT
6) Sequence classification / Naive Bayes classifier for Sentiment analysis
7) Relation Extraction



\hfill *Total Periods: 45*

** COURSE OUTCOMES
Upon the completion of the course the students should be able to 
1. Apply text pre processing techniques and build the language models
   (K3)
2. Explain the basic levels of knowledge namely word level and syntax level in language processing (K2)
3. Apply computational methods in lexical and vector semantics (K3)
4. Apply NLP techniques for discourse processing and build machine translation systems using deep learning (K3)
5. Apply learning methodologies for different NLP applications (K3)
6. Develop any NLP application by using NLP techniques and learning
   methodologies (K4).

** TEXT BOOKS
1. Daniel Jurafsky and James H Martin, ``Speech and Language
   Processing: An introduction to Natural Language Processing,
   Computational Linguistics and Speech Recognition'', 2nd Edition,
   Prentice Hall, 2008.
2. https://web.stanford.edu/~jurafsky/slp3/


** REFERENCES
1. Christopher D Manning, Hinrich Schutze, ``Foundations of
   Statistical Natural Language Processing'', MIT Press, 1999.
2. Steven Bird, Ewan Klien and Edward Loper, Natural Language Processing with Python,
   O'Reilly, 2009.
3. Nitin Indurkhya, Fred J Damerau, "Handbook of Natural Language
   Processing", 2nd Edition, CRC Press, 2010.
4. Yoav Goldberg, "Neural Network Methods for Natural Language
   Processing", Synthesis Lectures on Human Language Technologies,
   Morgan & Claypool publishers, 2017.
5. Li Deng, Yang Liu, "Deep Learning in Natural Language Processing", 
   Springer, 2018
6. Taweh Beysolow II, "Applied Natural Language Processing - Implementing 
   Machine Learning and Deep Learning Algorithms for Natural Language Processing", 
   Apress, 2018
7. NLTK -- Natural Language Tool Kit - http://www.nltk.org/
8. http://nlp-iiith.vlabs.ac.in/
9. https://www.tensorflow.org/tutorials/text/nmt_with_attention


** CO TO PO/PSO MAPPING 

| PO/PSO | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 1 | 2 | 3 |
|--------+---+---+---+---+---+---+---+---+---+----+----+----+---+---+---|
| CO1    | 3 | 3 | 2 | 1 | 0 | 0 | 0 | 0 | 0 |  1 |  0 |  0 | 3 | 0 | 0 |
| CO2    | 2 | 3 | 2 | 1 | 1 | 0 | 0 | 0 | 0 |  1 |  0 |  2 | 3 | 0 | 0 |
| CO3    | 1 | 1 | 2 | 1 | 1 | 0 | 0 | 0 | 0 |  1 |  0 |  2 | 3 | 0 | 0 |
| CO4    | 3 | 3 | 2 | 1 | 1 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 3 | 0 | 0 |
| CO5    | 1 | 2 | 2 | 0 | 0 | 0 | 0 | 0 | 0 |  1 |  0 |  1 | 3 | 0 | 0 |
| CO6    | 3 | 3 | 3 | 3 | 2 | 1 | 1 | 1 | 3 |  2 |  0 |  3 | 3 | 2 | 3 |
|--------+---+---+---+---+---+---+---+---+---+----+----+----+---+---+---|
| Course | 3 | 3 | 3 | 2 | 1 | 1 | 1 | 1 | 1 |  1 |  0 |  2 | 3 | 1 | 1 |

# | Score          | 13 | 15 | 13 | 7 | 5 | 0 | 0 | 1 | 3 |  7 |  0 |  9 | 18 | 2 | 3 |
